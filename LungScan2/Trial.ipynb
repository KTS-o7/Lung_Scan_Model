{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os, re, time, tqdm # import utility libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet import ResNet152, preprocess_input\n",
    "\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import LSTM, GRU, Conv2D, MaxPooling2D, Dense, Activation, Flatten\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dirname, _, filenames in os.walk('path for images'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory with training covid pictures\n",
    "train_covid_dir = os.path.join('Path to/CT_COVID')\n",
    "\n",
    "# Directory with training human pictures\n",
    "train_non_covid_dir = os.path.join('Path to/CT_NonCOVID')\n",
    "\n",
    "print(train_covid_dir)\n",
    "print(train_non_covid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print images of covid images\n",
    "train_covid_names = os.listdir(train_covid_dir)\n",
    "print(f'TRAIN SET COVID: {train_covid_names[:10]}')\n",
    "\n",
    "print('')\n",
    "\n",
    "# print names of non covid images\n",
    "train_non_covid_names = os.listdir(train_non_covid_dir)\n",
    "print(f'TRAIN SET NON-COVID: {train_non_covid_names[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing total number of images in each directory\n",
    "print(f'total training covid images: {len(os.listdir(train_covid_dir))}')\n",
    "print(f'total training non-covid images: {len(os.listdir(train_non_covid_dir))}')\n",
    "\n",
    "covid = len(os.listdir(train_covid_dir))\n",
    "non_covid = len(os.listdir(train_non_covid_dir))\n",
    "print('Total CT-scanned images in dataset: %s' % str(covid + non_covid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parameters for our plot; we will output images in a 4x4 configuration\n",
    "nrows, ncols = 4, 4\n",
    "\n",
    "# index for iterating over images\n",
    "pic_index = 0\n",
    "\n",
    "# setting up matplotlib fig, and size it to fit 4x4 pics\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols * 4, nrows * 4)\n",
    "\n",
    "pic_index += 8\n",
    "next_normal_pix = [os.path.join(train_covid_dir, fname) \n",
    "                for fname in train_covid_names[pic_index-8:pic_index]]\n",
    "next_malig_pix = [os.path.join(train_non_covid_dir, fname) \n",
    "                for fname in train_non_covid_names[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_normal_pix + next_malig_pix):\n",
    "    \n",
    "  # setting up subplot; subplot indices start at 1\n",
    "  sp = plt.subplot(nrows, ncols, i + 1)\n",
    "  sp.axis('Off') # don't show axes (or gridlines)\n",
    "\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.imshow(img)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Path to training data'\n",
    "X, Y = 224, 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# setting up the data generator for train and validation splits\n",
    "datagen = ImageDataGenerator(rescale=1/255.0, validation_split=0.2,\n",
    "                             rotation_range=40,\n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             fill_mode='nearest'\n",
    "                            )\n",
    "\n",
    "# test_datagen = ImageDataGenerator(rescale=1/255.0)\n",
    "\n",
    "training_data = datagen.flow_from_directory(data_dir,\n",
    "                                    class_mode = \"binary\",\n",
    "                                    target_size = (X, Y),\n",
    "                                    color_mode=\"rgb\",\n",
    "                                    batch_size = BATCH_SIZE, \n",
    "                                    shuffle = False,\n",
    "                                    subset='training',\n",
    "                                    seed = 42\n",
    "                                    )\n",
    "\n",
    "validation_data = datagen.flow_from_directory(data_dir,\n",
    "                                      class_mode = \"binary\",\n",
    "                                      target_size = (X, Y),\n",
    "                                      color_mode=\"rgb\",\n",
    "                                      batch_size = BATCH_SIZE, \n",
    "                                      shuffle = False,\n",
    "                                      subset='validation',\n",
    "                                      seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def nn_model():\n",
    "    '''\n",
    "    A function to make a deep convolutional neural network with convolutional, pooling and \n",
    "    fully-connected layers\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, (3, 3), padding='same', activation=tf.nn.relu, \n",
    "                     input_shape=(X, Y, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation=tf.nn.relu))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation=tf.nn.relu))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = nn_model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up early stopping to stop training when max acc is reached\n",
    "# we are using validation loss as parameter for monitoring the training process\n",
    "\n",
    "# setup an early stopping object for monitoring model performance\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=3)\n",
    "\n",
    "# setting up the csv logger\n",
    "logger = CSVLogger('logs.csv', append=True)\n",
    "\n",
    "# fitting the model to training data and validating on validation data\n",
    "history = model.fit(\n",
    "    training_data,\n",
    "    validation_data=validation_data,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping, logger]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "vgg19 = VGG19(input_shape=(224, 224, 3),\n",
    "                      weights='imagenet',\n",
    "                      include_top=False)\n",
    "\n",
    "# swtting traimable to false\n",
    "for layer in vgg19.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(vgg19.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "prediction = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs=vgg19.input, outputs=prediction)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = GlobalAveragePooling2D()(vgg19.output)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "y = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_2 = Model(inputs=vgg19.input, outputs=y)\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up early stopping to stop training when max acc is reached\n",
    "# we are using validation loss as parameter for monitoring the training process\n",
    "\n",
    "# fitting the model to training data and validating on validation data\n",
    "history = model.fit(\n",
    "    training_data,\n",
    "    validation_data=validation_data,\n",
    "    epochs=60,\n",
    "    callbacks=[logger]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = history.history\n",
    "plt.style.use('_classic_test_patch')\n",
    "\n",
    "epochs = range(1, len(hist['loss']) + 1)\n",
    "acc = hist['accuracy']\n",
    "loss = hist['loss']\n",
    "val_acc = hist['val_accuracy']\n",
    "val_loss = hist['val_loss']\n",
    "\n",
    "# plot loss versus accuracy\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(epochs, acc, label='accuracy')\n",
    "plt.plot(epochs, val_acc, label='val_acc')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# plot validation versus accuract\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(epochs, loss, label='loss', color='g')\n",
    "plt.plot(epochs, val_loss, label='val_loss', color='r')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"modelOne.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"path to save the trained model/modelOne.h5\")\n",
    "print(\"Model Saved to the disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"path to load the /modelOne.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread(\"Path to read test image\")\n",
    "print(type(image))\n",
    "image = cv2.resize(image, (224, 224))\n",
    "print(type(image))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(image.reshape(1, 224, 224, 3))\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
